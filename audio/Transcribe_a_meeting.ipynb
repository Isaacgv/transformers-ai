{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribe a meeting\n",
        "\n",
        "## Speaker Diarization\n",
        "\n",
        "Speaker diarization (or diarisation) is the task of taking an unlabelled audio input and predicting “who spoke when”.\n",
        "\n"
      ],
      "metadata": {
        "id": "UUgZSEPjd0wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained speaker diarization model\n",
        "! pip install pyannote.audio"
      ],
      "metadata": {
        "id": "1pIHDqbTjzt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "\n",
        "diarization_pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization@2.1\", use_auth_token=True\n",
        ")"
      ],
      "metadata": {
        "id": "w_XnyxNgjv5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a sample of the LibriSpeech ASR dataset that consists of two different speakers that have been concatenated together to give a single audio file."
      ],
      "metadata": {
        "id": "j5Qb2xr7e6if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "concatenated_librispeech = load_dataset(\n",
        "    \"sanchit-gandhi/concatenated_librispeech\", split=\"train\", streaming=True\n",
        ")\n",
        "sample = next(iter(concatenated_librispeech))"
      ],
      "metadata": {
        "id": "pns8QU3he1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "uQUNjA4BfGY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass this audio file to the diarization model to get the speaker start / end times"
      ],
      "metadata": {
        "id": "-GticloNfMrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_tensor = torch.from_numpy(sample[\"audio\"][\"array\"][None, :]).float()\n",
        "outputs = diarization_pipeline(\n",
        "    {\"waveform\": input_tensor, \"sample_rate\": sample[\"audio\"][\"sampling_rate\"]}\n",
        ")\n",
        "\n",
        "outputs.for_json()[\"content\"]"
      ],
      "metadata": {
        "id": "AU9u30LjfEGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech transcription\n",
        "\n",
        "Use the Whisper model for our speech transcription system"
      ],
      "metadata": {
        "id": "JaDMhtOyfWGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "asr_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\",\n",
        ")"
      ],
      "metadata": {
        "id": "uG9SOSBEfYQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the transcription for our sample audio, returning the segment level timestamps as well so that we know the start / end times for each segment."
      ],
      "metadata": {
        "id": "QVNWlK6Qfpxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_pipeline(\n",
        "    sample[\"audio\"].copy(),\n",
        "    generate_kwargs={\"max_new_tokens\": 256},\n",
        "    return_timestamps=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Jgus2ae3fkpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speechbox\n",
        "Find the closest alignment between diarization and transcription timestamps by minimising the absolute distance between both."
      ],
      "metadata": {
        "id": "2eQJf2_cf2Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/huggingface/speechbox"
      ],
      "metadata": {
        "id": "t8TPoq6yf7oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our combined diarization plus transcription pipeline, by passing the diarization model and ASR model to the ASRDiarizationPipeline class"
      ],
      "metadata": {
        "id": "l-JaCfTdgHVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbox import ASRDiarizationPipeline\n",
        "\n",
        "pipeline = ASRDiarizationPipeline(\n",
        "    asr_pipeline=asr_pipeline, diarization_pipeline=diarization_pipeline\n",
        ")"
      ],
      "metadata": {
        "id": "y__dI1xwgCzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(sample[\"audio\"].copy())"
      ],
      "metadata": {
        "id": "r-q8b_rNgSPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Format the timestamps"
      ],
      "metadata": {
        "id": "PtjxJ8RCgZYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converts a tuple of timestamps to a string, rounded to a set number of decimal places\n",
        "def tuple_to_string(start_end_tuple, ndigits=1):\n",
        "    return str((round(start_end_tuple[0], ndigits), round(start_end_tuple[1], ndigits)))\n",
        "\n",
        "# combines the speaker id, timestamp and text information onto one line, and splits each speaker onto their own line for ease of reading\n",
        "def format_as_transcription(raw_segments):\n",
        "    return \"\\n\\n\".join(\n",
        "        [\n",
        "            chunk[\"speaker\"] + \" \" + tuple_to_string(chunk[\"timestamp\"]) + chunk[\"text\"]\n",
        "            for chunk in raw_segments\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "vyztkoJngWwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipeline(sample[\"audio\"].copy())\n",
        "\n",
        "format_as_transcription(outputs)"
      ],
      "metadata": {
        "id": "qQBG-mT4gncT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}